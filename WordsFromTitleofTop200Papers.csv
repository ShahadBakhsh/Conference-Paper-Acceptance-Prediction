// https://chillee.github.io/OpenReviewExplorer/?conf=iclr2017
Understanding deep learning requires rethinking generalization,
Neural Architecture Search with Reinforcement Learning,
Towards Principled Methods for Training Generative Adversarial Networks,
Learning Graphical State Transitions,
Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data,
Deep Information Propagation,
Making Neural Programming Architectures Generalize via Recursion,
Discrete Variational Autoencoders,
SampleRNN: An Unconditional End-to-End Neural Audio Generation Model,
The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,
Amortised MAP Inference for Image Super-resolution,
Introspection:Accelerating Neural Network Training By Learning Weight Evolution,
Temporal Ensembling for Semi-Supervised Learning,
Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks,
Dynamic Coattention Networks For Question Answering,
End-to-end Optimized Image Compression,
Structured Attention Networks,
Snapshot Ensembles: Train,
Combining policy gradient and Q-learning,
On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,
A Learned Representation For Artistic Style,
Optimization as a Model for Few-Shot Learning,
Metacontrol for Adaptive Imagination-Based Optimization,
Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning,
Hierarchical Multiscale Recurrent Neural Networks,
Learning End-to-End Goal-Oriented Dialog,
Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations,
Learning to Act by Predicting the Future,
What does it take to generate natural textures?,
Unrolled Generative Adversarial Networks,
Density estimation using Real NVP,
Reinforcement Learning with Unsupervised Auxiliary Tasks,
Pointer Sentinel Mixture Models,
Capacity and Trainability in Recurrent Neural Networks,
Bidirectional Attention Flow for Machine Comprehension,
Calibrating Energy-based Generative Adversarial Networks,
Dropout with Expectation-linear Regularization,
Deep Learning with Dynamic Computation Graphs,
Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning,
Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning,
Sigma Delta Quantized Networks,
Learning through Dialogue Interactions by Asking Questions,
Revisiting Classifier Two-Sample Tests,
HyperNetworks,
Entropy-SGD: Biasing Gradient Descent Into Wide Valleys,
Adversarially Learned Inference,
Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights,
A Compositional Object-Based Approach to Learning Physical Dynamics,
Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement,
Improving Policy Gradient by Exploring Under-appreciated Rewards,
EPOpt: Learning Robust Neural Network Policies Using Model Ensembles,
Pruning Convolutional Neural Networks for Resource Efficient Inference,
Stochastic Neural Networks for Hierarchical Reinforcement Learning,
Recurrent Batch Normalization,
PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications,
Latent Sequence Decompositions,
Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization,
Learning Visual Servoing with Deep Features and Fitted Q-Iteration,
Energy-based Generative Adversarial Networks,
A Simple but Tough-to-Beat Baseline for Sentence Embeddings,
A recurrent neural network without chaos,
Variational Lossy Autoencoder,
Training Compressed Fully-Connected Networks with a Density-Diversity Penalty,
Towards a Neural Statistician,
Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic,
Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening,
Offline bilingual word vecto,
Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes,
Maximum Entropy Flow Networks,
Improving Neural Language Models with a Continuous Cache,
TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency,
Predicting Medications from Diagnostic Codes with Recurrent Neural Networks,
Learning to Compose Words into Sentences with Reinforcement Learning,
A Compare-Aggregate Model for Matching Text Sequences,
Learning to Generate Samples from Noise through Infusion Training,
Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,
Atte,
Learning Curve Prediction with Bayesian Neural Networks,
Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy,
Automatic Rule Extraction from Long Short Term Memory Networks,
Why Deep Neural Networks for Function Approximation?,
Learning in Implicit Generative Models,
Program Synthesis for Character Level Language Modeling,
Do Deep Convolutional Nets Really Need to be Deep and Convolutional?,
Multi-Agent Cooperation and the Emergence of (Natural) Language,
Frustratingly Short Attention Spans in Neural Language Modeling,
Batch Policy Gradient Methods for Improving Neural Conversation Models,
Recurrent Hidden Semi-Markov Model,
Steerable CNNs,
Loss-aware Binarization of Deep Networks,
Regularizing CNNs with Locally Constrained Decorrelations,
HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving,
Geometry of Polysemy,
Soft Weight-Sharing for Neural Network Compression,
Tracking the World State with Recurrent Entity Networks,
SGDR: Stochastic Gradient Descent with Warm Restarts,
Adversarial Feature Learning,
Query-Reduction Networks for Question Answering,
Semi-Supervised Classification with Graph Convolutional Networks,
Towards the Limit of Network Quantization,
Generalizing Skills with Semi-Supervised Reinforcement Learning,
Highway and Residual Networks learn Unrolled Iterative Estimation,
Visualizing Deep Neural Network Decisions: Prediction Difference Analysis,
Learning to superoptimize programs,
DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning,
Diet Networks: Thin Parameters for Fat Genomics,
Learning to Remember Rare Events,
Improving Generative Adversarial Networks with Denoising Feature Matching,
Deep Multi-task Representation Learning: A Tensor Factorisation Approach,
Lie-Access Neural Turing Machines,
Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music,
DSD: Dense-Sparse-Dense Training for Deep Neural Networks,
Stick-Breaking Variational Autoencoders,
Learning to Perform Physics Experiments via Deep Reinforcement Learning,
An Actor-Critic Algorithm for Sequence Prediction,
Lossy Image Compression with Compressive Autoencoders,
Inductive Bias of Deep Convolutional Networks through Pooling Geometry,
Hadamard Product for Low-rank Bilinear Pooling,
Support Regularized Sparse Coding and Its Fast Encoder,
Pruning Filters for Efficient ConvNets,
An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax,
Learning to Que,
Unsupervised Cross-Domain Image Generation,
PixelVAE: A Latent Variable Model for Natural Images,
Decomposing Motion and Content for Natural Video Sequence Prediction,
Recurrent Environment Simulators,
Data Noising as Smoothing in Neural Network Language Models,
Reasoning with Memory Augmented Neural Networks for Language Comprehension,
Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks,
Deep Probabilistic Programming,
Understanding Trainable Sparse Coding with Matrix Factorization,
Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks,
The Neural Noisy Channel,
Learning to Optimize,
Adversarial Training Methods for Semi-Supervised Text Classification,
Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,
Learning Features of Music From Scratch,
Generative Multi-Adversarial Networks,
FILTER SHAPING FOR CONVOLUTIONAL NEURAL NETWORKS,
Learning Recurrent Representations for Hierarchical Behavior Modeling,
Words or Characters? Fine-grained Gating for Reading Comprehension,
Efficient Vector Representation for Documents through Corruption,
Neuro-Symbolic Program Synthesis,
On the Quantitative Analysis of Decoder-Based Generative Models,
Efficient Softmax Approximation for GPUs,
Distributed Second-Order Optimization using Kronecker-Factored Approximations,
Optimal Binary Autoencoding with Pairwise Correlations,
Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations,
Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU,
Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses,
On Detecting Adversarial Perturbations,
Exploring Sparsity in Recurrent Neural Networks,
Categorical Reparameterization with Gumbel-Softmax,
Learning Invariant Representations Of Planar Curves,
Mollifying Networks,
LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation,
Deep Variational Information Bottleneck,
Online Bayesian Transfer Learning for Sequential Data Modeling,
Tree-structured decoding with doubly-recurrent neural networks,
Faster CNNs with Direct Sparse Convolutions and Guided Pruning,
Recurrent Mixture Density Network for Spatiotemporal Visual Attention,
Paleo: A Performance Model for Deep Neural Networks,
Learning to Navigate in Complex Environments,
Incorporating long-range consistency in CNN-based texture generation,
A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING,
Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks,
Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data,
Machine Comprehension Using Match-LSTM and Answer Pointer,
Learning a Natural Language Interface with Neural Programmer,
Sample Efficient Actor-Critic with Experience Replay,
beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,
Modularized Morphing of Neural Networks,
Adversarial Machine Learning at Scale,
Nonparametric Neural Networks,
Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning,
Quasi-Recurrent Neural Networks,
DeepCoder: Learning to Write Programs,
Efficient Representation of Low-Dimensional Manifolds using Deep Networks,
Identity Matters in Deep Learning,
Tighter bounds lead to improved classifiers,
Learning to Discover Sparse Graphical Models,
Neural Program Lattices,
Nonparametrically Learning Activation Functions in Deep Neural Nets,
Designing Neural Network Architectures using Reinforcement Learning,